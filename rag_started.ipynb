{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "76b36fe2",
      "metadata": {
        "id": "76b36fe2"
      },
      "outputs": [],
      "source": [
        "# reading from the pdf\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"attention.pdf\")\n",
        "doc = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f63724b1",
      "metadata": {
        "id": "f63724b1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter= RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200) \n",
        "# chunksize = 1000 character part/chunk \n",
        "# chunk overlap = do chunks ke beech me kitna common (repeat) text hoga. \n",
        "# Matlab agle chunk ki starting pichhle chunk\n",
        "# ke last 200 characters se hogi â€” taaki context na toote.\n",
        "documents = text_splitter.split_documents(doc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e691d267",
      "metadata": {
        "id": "e691d267"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# FAISS : Ek vector database hota hai jo fast search karne mein madad karta hai\n",
        "# HuggingFaceEmbeddings : Ye text ko number (vector) mein convert karta hai using pre-trained model\n",
        "\n",
        "# model 'all-mpnet-base-v2' Ye HuggingFace ka ek very good sentence embedding model hai.\n",
        "# ðŸ“Œ Kaam:\n",
        "# Text ya sentence ko dense vector (numbers) mein convert karta hai\n",
        "# Jo baad mein similarity search mein kaam aata hai (like: RAG, search bots)\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0XFEnu-iiwQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0XFEnu-iiwQ",
        "outputId": "36881f22-dd59-4c71-8551-212c1fa3cbc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x13d4c05cad0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore = FAISS.from_documents(documents[:20],embedding=hf)\n",
        "vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "eb517f2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "eb517f2c",
        "outputId": "d7686d17-8130-4650-e050-6b01baabfbbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"An attention function can be described as mapping  a query\"\n",
        "result = vectorstore.similarity_search(query)\n",
        "result[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9eSv2JgzmB7p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "9eSv2JgzmB7p",
        "outputId": "d794eb53-b7cc-40c6-fdd6-9b5b896d16c5"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the Ollama model\n",
        "llm = OllamaLLM(model=\"gemma3:1b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6541079f",
      "metadata": {
        "id": "6541079f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        " Answer the following question based only on the provided context.\n",
        " Think step by step before providing a detailed answer.\n",
        " I will tip you $1000 if the user finds the answer helpful,\n",
        " <context>\n",
        " {context}\n",
        " </context>\n",
        " Question: {input}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ojnabfD0nO12",
      "metadata": {
        "id": "ojnabfD0nO12"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain =create_stuff_documents_chain(llm=llm,prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cfbf6dde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfbf6dde",
        "outputId": "7ba8ffd9-3ce1-4290-f1d5-50aaa3558c9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000013D4C05CAD0>, search_kwargs={})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Retreivers : A retriever is an interaface that returns documents based on a query.\n",
        "    It is more general than a vector store. A retriever does not need to be able to store docuements,\n",
        "    only to return(retreive) them.\n",
        "\"\"\"\n",
        "\n",
        "retriever = vectorstore.as_retriever() # vectorstore is now connected to this variable and we can retrieve\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "de5564a1",
      "metadata": {
        "id": "de5564a1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Retrival chain: This chain takes in a user inquiry ,which is then passed to the retreiver to\n",
        " fetch relavent documents. Those decuments (and original inputs) are then passed to LLM to\n",
        " generate response\n",
        "\"\"\"\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "retrievel_chain = create_retrieval_chain(retriever,document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "gl8gcbGNwLm0",
      "metadata": {
        "id": "gl8gcbGNwLm0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Attention is used in the model to map a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. It allows the model to jointly attend to information from different representation subspaces at different positions.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response= retrievel_chain.invoke({\"input\":\"What is attention\"})\n",
        "response['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "xuzOnIrqwLjt",
      "metadata": {
        "id": "xuzOnIrqwLjt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'An attention function can be described as mapping  a query',\n",
              " 'context': [Document(id='85719649-4d5d-4e2c-a4db-277ace2e90f7', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
              "  Document(id='2f4d5f37-b133-4a68-9d29-0ee9e0fda870', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by âˆšdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\nâˆšdk\\n)V (1)'),\n",
              "  Document(id='52dd755d-add4-46de-b60e-8395f07918b8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
              "  Document(id='549f7e93-4397-4d6f-ba47-5113706eae46', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1âˆšdk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q Â· k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4')],\n",
              " 'answer': 'mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\\n'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MHVCXItfwLhZ",
      "metadata": {
        "id": "MHVCXItfwLhZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dOtjy34ftWfM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "dOtjy34ftWfM",
        "outputId": "5a82e5ee-b73d-405e-9ed2-40f12053c71f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
